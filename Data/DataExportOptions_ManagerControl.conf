# Required: Directory to save the schema files
OutputDir=F:\Cached_DBSchema_SQLServer_to_PgSql

# Required: Database server name; assumed to be Microsoft SQL Server unless /PgUser is provided (or PgUser is defined in the .conf file)
Server=Proteinseqs

# Database username; ignored if connecting to SQL Server with integrated authentication (default). Required if connecting to PostgreSQL
# DBUser=

# Password for the database user; ignored if connecting to SQL Server with integrated authentication
# For PostgreSQL, you can create a file named pgpass.conf at %APPDATA%\postgresql (or ~/.pgpass on Linux) to track passwords for PostgreSQL users\nList one entry per line, using the format server:port:database:username:password (see README.md for more info)
# DBPass=

# Existing schema file (DDL file from SSMS) to parse to rename columns based on information in the ColumnMap file
# Will also skip any tables or views with <skip> in the DataTables file
# The updated DDL file will end with _UpdatedColumnNames.sql or _UpdatedColumnAndTableNames.sql
ExistingDDL=

# With SQL Server databases, dump table data using COPY commands instead of INSERT INTO statements
# With PostgreSQL data, dump table data using pg_dump.exe and COPY commands
PgDump=True

# By default, the PgDump output file (_AllObjects_.sql) is deleted after it has been processed
# Set this to True to skip the deletion
# If any unhandled scripting commands are encountered, the _AllObjects_.sql file will not be deleted, even if KeepPgDumpFile is false
# KeepPgDumpFile=False

# Number of values to insert at a time when PgInsert is true for a table; only applicable when exporting data from SQL Server
PgInsertChunkSize=50000

# With SQL Server databases, while reading the TableDataToExport.txt file, default the PgInsert column to true, meaning table data will be exported via INSERT INTO statements using the syntax "ON CONFLICT (key_column) DO UPDATE SET"
# Ignored for PostgreSQL data
PgInsert=True

# Database username when connecting to a PostgreSQL server
# PgUser=

# PostgreSQL user password
# PgPass=

# Port to use for a PostgreSQL database
# PgPort=5432

# Database name (alternatively, use DBList for a list of databases)
DB=Manager_Control

# Comma separated list of database names
DBList=

# Prefix name for output directories; default name is DBSchema__DatabaseName
DirectoryPrefix=DBSchema__

# Skip exporting schema
NoSchema=True

# Disable creating a subdirectory for each database when copying data to the Sync directory
NoSubdirectory=False

# Create a subdirectory for the schema files for each database
CreateDBDirectories=True

# Export server settings, logins, and jobs
ServerInfo=False

# Text file with table names (one name per line) for which table data should be exported
# Also supports a multi-column, tab-delimited format, which allows for renaming tables and views
# Tab-delimited columns are:
# SourceTableName  TargetSchemaName  TargetTableName  PgInsert  KeyColumn(s)
DataTables=Manager_Control_Tables.tsv

# Text file mapping source column names to target column names. Tab-delimited columns are:
# SourceTableName  SourceColumnName  TargetColumnName
# TargetColumnName supports <skip> for not including the given column in the output file
ColumnMap=Manager_Control_Table_Columns.tsv

# Table name (or comma separated list of names) to restrict table export operations
# This is useful for exporting the data from just a single table (or a few tables)
# This parameter does not support reading names from a file; it only supports actual table names
# If a file was defined via the DataTables parameter, the list of tables loaded from that file will be filtered by this list (matching both SourceTableName and TargetTableName)
TableFilterList=

# Schema name (or comma separated list of names) of schema to skip. Useful if using partitioned tables
# SchemaSkipList=

# Text file used to define columns to skip when exporting data from tables
# The program auto-skips computed columns and timestamp columns when exporting data as PostgreSQL compatible INSERT INTO statements
# Tab-delimited columns are:
# Table  Column  Comment
TableDataColumnFilter=

# Text file used to filter data by date when exporting data from tables. Tab-delimited columns are:
# SourceTableName  DateColumnName  MinimumDate
TableDataDateFilter=

# Comma separated list of text to use to filter objects to export (at the command line, surround lists with double quotes)
# Supports RegEx symbols like ^, $, parentheses, and brackets
# Will not split on commas if the object name filter has square brackets
ObjectNameFilter=

# Text file with table names (one name per line) defining the order that data should be exported from tables
# The export order also dictates the order that tables will be listed in the shell script created for importing data into the target database
# The order with which data is imported is important if foreign key relationships are defined on tables prior to importing the data
# The data export order file should have table names only, without schema
# Tables not listed in this file will have their data exported alphabetically by table name (and will thus be appended alphabetically to the end of the shell script)
# If the first line is Table_Name, will assume that it is a header line (and will thus ignore it)
TableDataExportOrder=

# Default schema for exported tables and data. If undefined, use the original table's schema
DefaultSchema=mc

# In addition to table names defined by DataTables (or /Data), there are default tables which will have their data exported
# Disable the defaults using NoAutoData=True (or /NoAutoData)
NoAutoData=False

# Export data from every table in the database
# Will skip tables (and views) that have <skip> in the DataTables file
# Ignored if NoTableData is true
ExportAllData=True

# Maximum number of rows of data to export; defaults to 1000
# Use 0 to export all rows
MaxRows=0

# Use NoTableData=True (or /NoTableData) to prevent any table data from being exported
# This parameter is useful when processing an existing DDL file with ExistingDDL
NoTableData=False

# When exporting data, if this is true, include commands to disable triggers prior to inserting data into tables, then re-enable triggers after inserting the data; ignored if PgInsert is true
IncludeDisableTriggerCommands=False

# Generate a bash script for loading table data
ScriptLoad=True

# Auto changes column names from Upper_Case and UpperCase to lower_case when exporting table data
# Also used for table names when exporting data
# Entries in the DataTables and ColumnMap files will override auto-generated snake_case names
SnakeCase=True

# Copy new/changed files from the output directory to an alternative directory
# This is advantageous to prevent file timestamps from getting updated every time the schema is exported
# Sync=

# Auto-update any new or changed files using Git
# Git=False

# Auto-update any new or changed files using Subversion
# Svn=False

# Auto-update any new or changed files using Mercurial
# Hg=False

# Commit any updates to the repository
# Commit=False

# Log messages to a file; specify a base log file name using /LogFile:LogFileName
# CreateLogFile=False

# Base log file name (the actual name will include today's date); defaults to DB_Schema_Export_Tool
# BaseLogFileName=

# Specify the directory to save the log file in
# By default, the log file is created in the current working directory
# LogDir=

# Count the number of database objects that would be exported
Preview=False

# Show (but do not log) export stats
# Stats=False

# Show additional debug messages, both at the console and in the log file
# Trace=False

