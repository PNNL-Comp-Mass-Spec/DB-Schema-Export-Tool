# Required: Directory to save the schema files
OutputDir=F:\Cached_DBSchema_PgSql

# Required: Database server name; assumed to be Microsoft SQL Server unless /PgUser is provided (or PgUser is defined in the .conf file)
Server=prismdb2

# Database username; ignored if connecting to SQL Server with integrated authentication (default). Required if connecting to PostgreSQL
# DBUser=

# Password for the database user; ignored if connecting to SQL Server with integrated authentication
# For PostgreSQL, you can create a file named pgpass.conf at %APPDATA%\postgresql (or ~/.pgpass on Linux) to track passwords for PostgreSQL users\nList one entry per line, using the format server:port:database:username:password (see README.md for more info)
# DBPass=

# Existing schema file (DDL file from SSMS) to parse to rename columns based on information in the ColumnMap file
# Will also skip any tables or views with <skip> in the DataTables file
# The updated DDL file will end with _UpdatedColumnNames.sql or _UpdatedColumnAndTableNames.sql
ExistingDDL=

# When true and exporting data from a SQL Server database, write table data using COPY commands (if PgInsert is false); however, if PgInsert is true, use INSERT INTO statements using the syntax "ON CONFLICT (key_column) DO UPDATE SET"
# When true and exporting data from a PostgreSQL database, dump table data using pg_dump.exe and COPY commands
PgDump=True

# By default, the PgDump output file (_AllObjects_.sql) is deleted after it has been processed
# Set this to True to skip the deletion
# If any unhandled scripting commands are encountered, the _AllObjects_.sql file will not be deleted, even if KeepPgDumpFile is false
KeepPgDumpFile=False

# Number of values to insert at a time when PgInsert is true for a table; only applicable when exporting data from SQL Server
PgInsertChunkSize=50000

# When true and exporting data from a SQL Server database, while reading the TableDataToExport.txt file, default the PgInsert column to true, meaning table data will be exported via INSERT INTO statements using the syntax "ON CONFLICT (key_column) DO UPDATE SET"
# When false, if PgDump is true, export data using "COPY t_table (column1, column2, column3) FROM stdin;" followed by a list of tab-delimited rows of data, otherwise, export data using SQL Server compatible INSERT INTO statements
# Ignored for PostgreSQL data
PgInsert=False

# Database username when connecting to a PostgreSQL server
PgUser=d3l243

# PostgreSQL user password
PgPass=

# Port to use for a PostgreSQL database
# PgPort=5432

# Database name (alternatively, use DBList for a list of databases)
DB=

# Comma separated list of database names
DBList=dms,d3l243,pgwatch2_metrics

# Prefix name for output directories; default name is DBSchema__DatabaseName
DirectoryPrefix=DBSchema__

# Skip exporting schema
NoSchema=False

# Disable creating a subdirectory for each database when copying data to the Sync directory
NoSubdirectory=False

# Create a subdirectory for the schema files for each database
CreateDBDirectories=True

# Export server settings, logins, and jobs
ServerInfo=True

# Text file with table names (one name per line) for which table data should be exported
# Also supports a multi-column, tab-delimited format, which allows for renaming tables and views
# Tab-delimited columns are:
# SourceTableName  TargetSchemaName  TargetTableName  PgInsert  KeyColumn(s)
DataTables=PgSQL_DMS_Data_Tables.txt

# Text file mapping source column names to target column names. Tab-delimited columns are:
# SourceTableName  SourceColumnName  TargetColumnName
# TargetColumnName supports <skip> for not including the given column in the output file
ColumnMap=

# Table name (or comma separated list of names) to restrict table export operations
# This is useful for exporting the data from just a single table (or a few tables)
# This parameter does not support reading names from a file; it only supports actual table names
# If a file was defined via the DataTables parameter, the list of tables loaded from that file will be filtered by this list (matching both SourceTableName and TargetTableName)
TableFilterList=

# Schema name (or comma separated list of names) of schema to skip. Useful if using partitioned tables
SchemaSkipList=subpartitions

# Text file used to define columns to skip when exporting data from tables
# The program auto-skips computed columns and timestamp columns when exporting data as PostgreSQL compatible INSERT INTO statements
# Tab-delimited columns are:
# Table  Column  Comment
TableDataColumnFilter=

# Text file used to filter data by date when exporting data from tables. Tab-delimited columns are:
# SourceTableName  DateColumnName  MinimumDate
TableDataDateFilter=

# Comma separated list of text to use to filter objects to export (at the command line, surround lists with double quotes)
# Supports RegEx symbols like ^, $, parentheses, and brackets
# Will not split on commas if the object name filter has square brackets
ObjectNameFilter=

# Text file with table names (one name per line) defining the order that data should be exported from tables
# The export order also dictates the order that tables will be listed in the shell script created for importing data into the target database
# The order with which data is imported is important if foreign key relationships are defined on tables prior to importing the data
# The data export order file should have table names only, without schema
# Tables not listed in this file will have their data exported alphabetically by table name (and will thus be appended alphabetically to the end of the shell script)
# If the first line is Table_Name, will assume that it is a header line (and will thus ignore it)
TableDataExportOrder=

# Default schema for exported tables and data. If undefined, use the original table's schema
DefaultSchema=

# In addition to table names defined by DataTables (or /Data), there are default tables which will have their data exported
# Disable the defaults using NoAutoData=True (or /NoAutoData)
NoAutoData=False

# Export data from every table in the database, creating one file per table
# Will skip tables (and views) that have <skip> in the DataTables file
# Ignored if NoTableData is true
ExportAllData=False

# Maximum number of rows of data to export; defaults to 1000
# Use 0 to export all rows
MaxRows=1000

# When true, create files with commands to delete extra rows in database tables (provided the table has 1-column based primary key)
# If ScriptLoad is true, the shell script will use these files to delete extra rows prior to importing new data for each table
DeleteExtraRows=False

# Table name (or comma separated list of names) to truncate when deleting extra rows prior to loading new data
# This is useful for large tables where the default DELETE FROM queries run very slowly
# This parameter does not support reading names from a file; it only supports actual table names
# Ignored if DeleteExtraRows is False
ForceTruncateTableList=

# Use NoTableData=True (or /NoTableData) to prevent any table data from being exported
# This parameter is useful when processing an existing DDL file with ExistingDDL
NoTableData=False

# When exporting data, if this is true, include commands to disable triggers prior to inserting/deleting table data,
# then re-enable triggers after inserting/deleting data; ignored if PgInsert is true since session_replication_role is set to "replica" prior to inserting/deleting data
IncludeDisableTriggerCommands=False

# Generate a bash script for loading table data
ScriptLoad=False

# Username to use when calling psql in the bash script for loading table data
ScriptLoadUser=

# Database name to use when calling psql in the bash script for loading table data
ScriptLoadDatabase=dms

# Host name to use when calling psql in the bash script for loading table data
ScriptLoadHost=localhost

# Port number to use when calling psql in the bash script
ScriptLoadPort=5432

# When true, change the PostgreSQL server setting 'log_min_duration_statement' to -1 prior to loading data (the user must be a superuser);
# this will stop the server logging long queries to the log file, which is important when populating large tables
# After data loading is complete, the value for 'log_min_duration_statement' will be set to 5000 (adjustable using MinLogDurationAfterLoad)
DisableStatementLogging=True

# Value to use for 'log_min_duration_statement' after data loading is complete;
# Use -1 to disable logging, 0 to log all statements and their durations, or > 0 to log statements running at least this number of milliseconds
MinLogDurationAfterLoad=5000

# Auto changes column names from Upper_Case and UpperCase to lower_case when exporting table data
# Also used for table names when exporting data
# Entries in the DataTables and ColumnMap files will override auto-generated snake_case names
SnakeCase=False

# Copy new/changed files from the output directory to an alternative directory
# This is advantageous to prevent file timestamps from getting updated every time the schema is exported
Sync=F:\Documents\Projects\DataMining\Database_Schema\DMS_PgSQL

# Auto-update any new or changed files using Git
Git=True

# Auto-update any new or changed files using Subversion
# Svn=False

# Auto-update any new or changed files using Mercurial
# Hg=False

# Commit any updates to the repository
Commit=False

# Log messages to a file; specify a base log file name using /LogFile:LogFileName
CreateLogFile=True

# Base log file name (the actual name will include today's date); defaults to DB_Schema_Export_Tool
# BaseLogFileName=

# Specify the directory to save the log file in
# By default, the log file is created in the current working directory
LogDir=Logs

# Count the number of database objects that would be exported
Preview=False

# Show (but do not log) export stats
Stats=False

# Show additional debug messages, both at the console and in the log file
# Trace=False

